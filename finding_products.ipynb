{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f576c4d4-f1e4-443c-b8e0-695b3a25ba8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Welcome to the cloud products notebook!\n",
    "#### **Audience:** Anybody with a computer and interest in cloud data.\n",
    "#### **Intent:** Build familiarity with the process of requesting NOAA datasets from the AWS, Azure, and Google clouds.\n",
    "This is a short tutorial on using the \"requests\" library, in conjunction with data from a Cloud Service Provider (CSP; either AWS, Azure, or Google), to find and use a desired dataset. The example used in this notebook is GOES-16 data, but a familiarity with the structure of the code required to request data from the cloud will allow the user to alter this code to be applicable to any open-source NOAA dataset in the cloud. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0896fdc-ece5-4ea9-9a8e-459e5ec7b5e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# AWS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89794c5d-076a-4ddb-994c-417b5444fe69",
   "metadata": {
    "tags": []
   },
   "source": [
    "How to find datasets on AWS:     \n",
    "1. Go to the NODD page (https://www.noaa.gov/nodd/datasets), locate your dataset, and click on the \"Amazon Web Services\" link.   \n",
    "2. Under \"Resources on AWS\", locate your desired bucket. Then, click \"Browse Bucket.\"\n",
    "3. Click through your desired product, date, and time, until you see the list of files.\n",
    "4. To the right of the \"AWS S3 Explorer\" text on the top of your screen, you'll see the file path you followed. Adjust the name_string variable in the cell below to reflect your new product path. You can either choose generic variable names, then fill them in with your specific request (as shown below), or copy the specific file path directly into name_string, and delete the generic variable definitions (i.e. name_string = \"noaa-goes18/ABI-L2-ACHAC/2023/003/02\").      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca492bd3-cb87-43a5-8dd3-2d76cfe195a6",
   "metadata": {},
   "source": [
    "**For this tutorial:** GOES (16, 17, 18) buckets: https://registry.opendata.aws/noaa-goes/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f1a609-caf9-4652-bdc4-7f91edcb85b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import s3fs\n",
    "import netCDF4\n",
    "\n",
    "# Select your desired variables by looking at the product documentation (see instructions above!)\n",
    "satellite = \"noaa-goes16\"\n",
    "product = \"ABI-L2-LSTM\"\n",
    "year = \"2023\"\n",
    "day_of_year = \"004\"\n",
    "hour = \"17\"\n",
    "\n",
    "name_string = f'{satellite}/{product}/{year}/{day_of_year}/{hour}'\n",
    "\n",
    "# Using name_string as a file path, search for all files that match your criteria.\n",
    "aws = s3fs.S3FileSystem(anon=True)\n",
    "data_files = aws.ls(name_string, refresh=True)\n",
    "\n",
    "# See which files your search produced, and return the file name.\n",
    "# Then, use the requests library to find each file that met your search criteria. \n",
    "for f in data_files:\n",
    "    \n",
    "    print(f) \n",
    "    \n",
    "    fname = f[len(satellite)+1:]\n",
    "    resp = requests.get(f'https://{satellite}.s3.amazonaws.com/{fname}')\n",
    "    \n",
    "    # Read the file in as a netCDF file, and you may begin analysis.\n",
    "    nc = netCDF4.Dataset(fname, memory = resp.content)\n",
    "    \n",
    "    # your analysis here.\n",
    "\n",
    "# For every run of the above loop, your data file will be overwritten because the data is in memory.\n",
    "# So, either perform your analysis within the 'for' loop or make each netCDF file available outside of the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06daf37c-d5fe-419d-a43e-3e225e706cd7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Azure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476c0d7b-e118-4325-8368-f1304f303041",
   "metadata": {},
   "source": [
    "How to find datasets on Azure:     \n",
    "1. Go to the NODD page (https://www.noaa.gov/nodd/datasets), locate your dataset, and click on the \"Microsoft Azure\" link.   \n",
    "2. Scroll down for documentation of product availability and file path. \n",
    "3. Adjust the name_string variable in the cell below to reflect your new product path. You can either choose generic variable names, then fill them in with your specific request (as shown below), or copy the specific file path directly into name_string, and delete the generic variable definitions (i.e. name_string = \"ABI-L2-ACHAC/2023/003/02/\").      \n",
    "4. Make sure to change the container name from the \"satellite\" variable to the name of the container as documented on Azure's dataset page. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfffbd4f-b1a1-404f-8c1d-7cd754b6cd69",
   "metadata": {
    "tags": []
   },
   "source": [
    "**For this tutorial:** GOES (16, 17, 18) buckets: https://microsoft.github.io/AIforEarthDataSets/data/goes-r.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff1b6d3-4013-4164-9228-95fce06f74a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from azure.storage.blob import ContainerClient\n",
    "import netCDF4\n",
    "\n",
    "# Select your desired variables by looking at the product documentation (see instructions above!)\n",
    "satellite = \"noaa-goes16\"\n",
    "product = \"ABI-L2-LSTM\"\n",
    "year = \"2023\"\n",
    "day_of_year = \"004\"\n",
    "hour = \"17\"\n",
    "\n",
    "name_string = f'{product}/{year}/{day_of_year}/{hour}/'\n",
    "\n",
    "# Using name_string as a file path, search for all files that match your criteria.\n",
    "container_client = ContainerClient(account_url='https://goeseuwest.blob.core.windows.net', \n",
    "                                                     container_name=satellite, credential=None)\n",
    "\n",
    "# See which files your search produced, and return the file name.\n",
    "# Then, use the requests library to find each file that met your search criteria. \n",
    "for f in container_client.walk_blobs(name_starts_with=name_string, delimiter='/'):\n",
    "    \n",
    "    print(f.name)\n",
    "    \n",
    "    fname = f.name\n",
    "    resp = requests.get(f'https://goeseuwest.blob.core.windows.net/{satellite}/{fname}')\n",
    "    \n",
    "    # Read the file in as a netCDF file, and you may begin analysis.\n",
    "    nc = netCDF4.Dataset(fname, memory = resp.content)\n",
    "    \n",
    "    # your analysis here\n",
    "    \n",
    "# For every run of the above loop, your data file will be overwritten because the data is in memory.\n",
    "# So, either perform your analysis within the 'for' loop or make each netCDF file available outside of the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d799f8-985d-4d93-91cf-a542366525ce",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Google"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1baeb8-d471-4fa0-a26b-a43357ea7593",
   "metadata": {},
   "source": [
    "How to find datasets on Google:     \n",
    "1. Go to the NODD page (https://www.noaa.gov/nodd/datasets), locate your dataset, and click on the \"Google\" link.   \n",
    "2. From the Google product page, click the blue \"VIEW DATASET\" button. From here, you may select a project and discover your desired file path.\n",
    "3. Adjust the name_string variable in the cell below to reflect your new product path. You can either choose generic variable names, then fill them in with your specific request (as shown below), or copy the specific file path directly into name_string, and delete the generic variable definitions (i.e. name_string = \"ABI-L2-ACHAC/2023/003/02/\").      \n",
    "4. Be sure to change the bucket name (in this example, the bucket name is stored in the \"satellite\" variable) to reflect your product choice. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd57d98-25ca-4b3e-bef5-702e5c513384",
   "metadata": {
    "tags": []
   },
   "source": [
    "**For this tutorial:** GOES (16, 17, 18) buckets: https://console.cloud.google.com/marketplace/product/noaa-public/goes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00d0f79-d6bb-430f-8825-e8743b22ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from google.cloud import storage\n",
    "import netCDF4\n",
    "\n",
    "# Select your desired variables by looking at the product documentation (see instructions above!)\n",
    "satellite = \"gcp-public-data-goes-16\"\n",
    "product = \"ABI-L2-LSTM\"\n",
    "year = \"2023\"\n",
    "day_of_year = \"004\"\n",
    "hour = \"17\"\n",
    "\n",
    "name_string = f'{product}/{year}/{day_of_year}/{hour}/'\n",
    "\n",
    "# Using name_string as a file path, search for all files that match your criteria.\n",
    "client = storage.Client.create_anonymous_client()\n",
    "bucket = client.bucket(bucket_name = satellite)\n",
    "\n",
    "blobs = bucket.list_blobs(prefix = name_string, delimiter = \"/\")\n",
    "response = blobs._get_next_page_response()\n",
    "\n",
    "# See which files your search produced, and return the file name and url.\n",
    "# Then, use the requests library to find each file that met your search criteria. \n",
    "for f in range(0, len(response[\"items\"])):\n",
    "    print(((response[\"items\"])[f])[\"name\"])\n",
    "    \n",
    "    url_link = ((response[\"items\"])[f])[\"mediaLink\"]\n",
    "    fname = ((response[\"items\"])[f])[\"name\"]\n",
    "    resp = requests.get(url_link)\n",
    "    \n",
    "    # Read the file in as a netCDF file, and you may begin analysis.\\\n",
    "    nc = netCDF4.Dataset(fname, memory = resp.content)\n",
    "    \n",
    "    # your analysis here\n",
    "\n",
    "# For every run of the above loop, your data file will be overwritten because the data is in memory.\n",
    "# So, either perform your analysis within the 'for' loop or make each netCDF file available outside of the loop."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
